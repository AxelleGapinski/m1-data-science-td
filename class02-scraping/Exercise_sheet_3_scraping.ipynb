{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b721495e",
   "metadata": {},
   "source": [
    "# Exercise Sheet \\#3\n",
    "\n",
    "\n",
    "## Exercise 1. Quotes: manual scraping\n",
    "\n",
    "In this exercise, you are required to compile a dataset of biographies taken from http://quotes.toscrape.com.\n",
    "Recall this website displays 10 quotes per page, together with a link to their author's biography. This will be a step by step guide.\n",
    "\n",
    "#### 1.1 Getting URLs of authors' pages\n",
    "\n",
    "To get a list of URLs pointing at author pages, you will process quotes' pages. \n",
    "\n",
    "To do so, first complete the function get_links below which expects as parameter:\n",
    "\n",
    "* `url` the URL of a page from quotes.toscrape.com\n",
    "\n",
    "and returns:\n",
    "\n",
    "* `authors` the list of links to author pages contained in the given quotes' page (beware of duplicates!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2acd76fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import requests, re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BASE_URL = 'http://quotes.toscrape.com'\n",
    "\n",
    "def get_links(url):\n",
    "    authors = []\n",
    "    # Get page located at url:\n",
    "    \n",
    "    #Get all links corresponding to authors:\n",
    "    \n",
    "    #Loop over these:\n",
    "    \n",
    "        #if a link is not in authors, add it:\n",
    "        \n",
    "    #Return results\n",
    "    return authors\n",
    "\n",
    "#Test:\n",
    "authors = get_links(BASE_URL)\n",
    "print(authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5770d1",
   "metadata": {},
   "source": [
    "#### 1.2 iterate over pages of quotes\n",
    "\n",
    "In a second step, fill the `collect` function below, which will iteratively collect author links. This function will take as input parameters:\n",
    "- `url`: the starting url from which to collect links,\n",
    "- `authors`: the list of links to be updated\n",
    "- `limit`: the number of pages to visit (default being `None`, which means visit all pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae83d7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "def collect(url, authors, limit=None):\n",
    "    #Add links contained in page located at url to the authors being computed\n",
    "    authors.extend([x for x in get_links(url) if x not in authors])\n",
    "    #If no limit is given or limit > 1\n",
    "\n",
    "        # Get page located at url:\n",
    "\n",
    "        # Get url of next page\n",
    "\n",
    "        # recursively collect links (if any)\n",
    "\n",
    "# Test\n",
    "authors = []\n",
    "collect(BASE_URL, authors, limit=2)\n",
    "print(authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c8631f",
   "metadata": {},
   "source": [
    "#### Question 1.3 : get actual biographies\n",
    "\n",
    "For each of the links computed in the previous question, retrieve the corresponding webpage and extract the biography it contains. To do so, fill the `get_biography` function below. It will feed a list of dictionaries of the following form:\n",
    "```python\n",
    "bios = [{name: '...', birth_date: '...', birth_place: '...', bio: '...'}, ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a9e3d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "def get_biography(url):\n",
    "    # Get page located at URL and parse it\n",
    "    \n",
    "    # Get name with BeautifulSoup\n",
    "    name = None\n",
    "    # Get birth date\n",
    "    birth_date = None\n",
    "    # Get birth place\n",
    "    birth_place= None\n",
    "    # Get bio\n",
    "    bio = None\n",
    "    return {'name':name, 'birth_date': birth_date, 'birth_place': birth_place, 'bio': bio}\n",
    "\n",
    "def get_bios(urls):\n",
    "    bios = []\n",
    "    for u in urls:\n",
    "        bios.append(get_biography(u))\n",
    "    return bios\n",
    "\n",
    "#Test\n",
    "bios=get_bios(authors)\n",
    "print(bios)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16092ec8",
   "metadata": {},
   "source": [
    "#### Question 1.4: save your dataset\n",
    "\n",
    "Finally, write a `save` function which takes as an input a list of biographies as computed above and save them in JSON on disk (the filename being an input parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "411b95fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save(filename, dataset):\n",
    "    # Open output file\n",
    "    # write data in JSON format\n",
    "    pass #remove when ready\n",
    "\n",
    "save('bios.json', bios)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b70760d",
   "metadata": {},
   "source": [
    "## Exercise 2. Let's use Scrapy now!\n",
    "\n",
    "Here the goal is to play with scrapy. Let's look at the wikipedia article https://en.wikipedia.org/wiki/List_of_French_artists. Let's say, we want to extract all names of artists from here with links to their corresponding wikipedia pages and the first paragraph about them.\n",
    "\n",
    "You will find a file called `Exercise_sheet_3_scrapy.py`. Can you fill in the gaps in this script?\n",
    "\n",
    "\n",
    "In addition to the Scrapy documentation I highly recommend you to look at possible selectors: https://www.w3schools.com/cssref/css_selectors.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "347bbbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.8.0-py2.py3-none-any.whl (272 kB)\n",
      "     ---------------------------------------- 0.0/272.9 kB ? eta -:--:--\n",
      "     - -------------------------------------- 10.2/272.9 kB ? eta -:--:--\n",
      "     - -------------------------------------- 10.2/272.9 kB ? eta -:--:--\n",
      "     - -------------------------------------- 10.2/272.9 kB ? eta -:--:--\n",
      "     ----------- ------------------------- 81.9/272.9 kB 459.5 kB/s eta 0:00:01\n",
      "     -------------- --------------------- 112.6/272.9 kB 469.7 kB/s eta 0:00:01\n",
      "     ------------------------- ---------- 194.6/272.9 kB 695.5 kB/s eta 0:00:01\n",
      "     -----------------------------------  266.2/272.9 kB 820.5 kB/s eta 0:00:01\n",
      "     ------------------------------------ 272.9/272.9 kB 765.9 kB/s eta 0:00:00\n",
      "Collecting Twisted>=18.9.0\n",
      "  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n",
      "     ---------------------------------------- 0.0/3.1 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.3/3.1 MB 5.7 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 0.5/3.1 MB 4.7 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 0.8/3.1 MB 4.5 MB/s eta 0:00:01\n",
      "     ----------- ---------------------------- 0.9/3.1 MB 4.1 MB/s eta 0:00:01\n",
      "     ----------- ---------------------------- 0.9/3.1 MB 4.1 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 1.1/3.1 MB 3.2 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 1.2/3.1 MB 3.3 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 1.5/3.1 MB 3.4 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 1.6/3.1 MB 3.4 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 1.7/3.1 MB 3.5 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 1.9/3.1 MB 3.4 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 1.9/3.1 MB 3.4 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 2.1/3.1 MB 3.3 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 2.2/3.1 MB 3.2 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 2.3/3.1 MB 3.1 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 2.4/3.1 MB 3.0 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 2.4/3.1 MB 3.0 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 2.4/3.1 MB 3.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 2.7/3.1 MB 2.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 2.8/3.1 MB 2.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 3.0/3.1 MB 2.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  3.1/3.1 MB 2.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  3.1/3.1 MB 2.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.1/3.1 MB 2.7 MB/s eta 0:00:00\n",
      "Collecting cryptography>=3.4.6\n",
      "  Downloading cryptography-39.0.2-cp36-abi3-win_amd64.whl (2.5 MB)\n",
      "     ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.1/2.5 MB 5.5 MB/s eta 0:00:01\n",
      "     -- ------------------------------------- 0.2/2.5 MB 2.1 MB/s eta 0:00:02\n",
      "     --- ------------------------------------ 0.2/2.5 MB 1.6 MB/s eta 0:00:02\n",
      "     --- ------------------------------------ 0.2/2.5 MB 1.6 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 0.3/2.5 MB 1.5 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 0.3/2.5 MB 1.5 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 0.3/2.5 MB 1.5 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 0.6/2.5 MB 1.7 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 0.7/2.5 MB 1.7 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 0.8/2.5 MB 1.7 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 1.0/2.5 MB 2.1 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 1.3/2.5 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 1.5/2.5 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 1.8/2.5 MB 2.7 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 1.9/2.5 MB 2.8 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 1.9/2.5 MB 2.8 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 1.9/2.5 MB 2.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 2.0/2.5 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 2.3/2.5 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.5 MB 2.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.5 MB 2.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.5/2.5 MB 2.3 MB/s eta 0:00:00\n",
      "Collecting cssselect>=0.9.1\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting itemloaders>=1.0.1\n",
      "  Downloading itemloaders-1.0.6-py3-none-any.whl (11 kB)\n",
      "Collecting parsel>=1.5.0\n",
      "  Downloading parsel-1.7.0-py2.py3-none-any.whl (14 kB)\n",
      "Collecting pyOpenSSL>=21.0.0\n",
      "  Downloading pyOpenSSL-23.0.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 0.0/57.3 kB ? eta -:--:--\n",
      "     ----------------------------------- ---- 51.2/57.3 kB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 57.3/57.3 kB 1.0 MB/s eta 0:00:00\n",
      "Collecting queuelib>=1.4.2\n",
      "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
      "Collecting service-identity>=18.1.0\n",
      "  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting w3lib>=1.17.0\n",
      "  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n",
      "Collecting zope.interface>=5.1.0\n",
      "  Downloading zope.interface-5.5.2-cp311-cp311-win_amd64.whl (211 kB)\n",
      "     ---------------------------------------- 0.0/211.7 kB ? eta -:--:--\n",
      "     -------------------------------------- - 204.8/211.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 211.7/211.7 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting protego>=0.1.15\n",
      "  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting itemadapter>=0.1.0\n",
      "  Downloading itemadapter-0.7.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\axelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scrapy) (67.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\axelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scrapy) (23.0)\n",
      "Collecting tldextract\n",
      "  Downloading tldextract-3.4.0-py3-none-any.whl (93 kB)\n",
      "     ---------------------------------------- 0.0/93.9 kB ? eta -:--:--\n",
      "     ---------------------------------------  92.2/93.9 kB 5.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 93.9/93.9 kB 1.8 MB/s eta 0:00:00\n",
      "Collecting lxml>=4.3.0\n",
      "  Using cached lxml-4.9.2-cp311-cp311-win_amd64.whl (3.8 MB)\n",
      "Collecting PyDispatcher>=2.0.5\n",
      "  Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\axelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cryptography>=3.4.6->scrapy) (1.15.1)\n",
      "Collecting jmespath>=0.9.5\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: six in c:\\users\\axelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from protego>=0.1.15->scrapy) (1.16.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\axelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (22.2.0)\n",
      "Collecting pyasn1-modules\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     ---------------------------------------- 0.0/155.3 kB ? eta -:--:--\n",
      "     -------------------------------------- 155.3/155.3 kB 4.5 MB/s eta 0:00:00\n",
      "Collecting pyasn1\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     ---------------------------------------- 0.0/77.1 kB ? eta -:--:--\n",
      "     ------------------------------------- -- 71.7/77.1 kB 3.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 77.1/77.1 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting constantly>=15.1\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting incremental>=21.3.0\n",
      "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting Automat>=0.8.0\n",
      "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "     ---------------------------------------- 0.0/74.6 kB ? eta -:--:--\n",
      "     -------------------------------------- - 71.7/74.6 kB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 74.6/74.6 kB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\axelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (4.5.0)\n",
      "Collecting twisted-iocpsupport<2,>=1.0.2\n",
      "  Downloading twisted-iocpsupport-1.0.2.tar.gz (10 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: idna in c:\\users\\axelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tldextract->scrapy) (3.4)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\axelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tldextract->scrapy) (2.28.2)\n",
      "Collecting requests-file>=1.4\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Collecting filelock>=3.0.8\n",
      "  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\axelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.12->cryptography>=3.4.6->scrapy) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\axelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\axelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\axelle\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2022.12.7)\n",
      "Building wheels for collected packages: twisted-iocpsupport\n",
      "  Building wheel for twisted-iocpsupport (pyproject.toml): started\n",
      "  Building wheel for twisted-iocpsupport (pyproject.toml): finished with status 'error'\n",
      "Failed to build twisted-iocpsupport\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for twisted-iocpsupport (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [5 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_ext\n",
      "  building 'twisted_iocpsupport.iocpsupport' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for twisted-iocpsupport\n",
      "ERROR: Could not build wheels for twisted-iocpsupport, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fee6b88",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scrapy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscrapy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mWikipediaSpider\u001b[39;00m(scrapy\u001b[38;5;241m.\u001b[39mSpider):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'scrapy'"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "import json\n",
    "\n",
    "class WikipediaSpider(scrapy.Spider):\n",
    "        name = \"wikipedia\"\n",
    "        \n",
    "        start_urls = [\"https://en.wikipedia.org/wiki/List_of_French_artists\"]\n",
    "        \n",
    "        def parse(self, response):\n",
    "            list_els = response.css('div.mw-parser-output>ul>li>a::attr(href)').getall() #get all links which are in lists on the page\n",
    "            list_see_also = response.css('div.mw-parser-output>ul:last-of-type>li>a::attr(href)').getall() #get the links in see also (they are not about the disctinct authors)\n",
    "            res_list = list(set(list_els) - set(list_see_also))\n",
    "            for link in res_list:\n",
    "                if link.startswith(\"/wiki/\"): #check that the link actually exists and is not red\n",
    "                    yield response.follow(link, callback=self.parse_artist)\n",
    "                    \n",
    "        def parse_artist(self, response):\n",
    "            #url = #get url of the page\n",
    "            name = response.css('span.mw-page-title-main::text').get() # get name of the artist\n",
    "            paragraph = response.css('div.mw-parser-output>p:nth-of-type(1)').xpath('string(.)').get() # get the first paragraph\n",
    "            yield {'url': url,\n",
    "                   'name': name,\n",
    "                   'paragraph': paragraph}\n",
    "            \n",
    "        if __name__=='__main__':\n",
    "            import scrapy.crawler\n",
    "            \n",
    "            process = scrapy.crawler.CrawlerProcess({\n",
    "                'USER_AGENT': 'Mozilla/5.0',\n",
    "                'FEEDS': {\n",
    "                    \"artists.json\": {\"format\": \"json\"},\n",
    "                },\n",
    "            \n",
    "            })\n",
    "            process.crawl(WikipediaSpider)\n",
    "            process.start()\n",
    "            process.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e81c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c24c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
